# Relatório de Estudos

**Nome do Estagiário:** Gustavo Monteiro Gomes Pires 
**Data:** 07/08/2024

**Módulos/Etapas Feitas:**  
1. Bases analíticas
2. Bases Transacionais
3. Big Query
4. Python
5. Apache Spark (PySpark)
6. Apache Beam
7. Google Dataflow
8. Apache Airflow

## Resumo dos módulos 

**Bases Analíticas**

São estruturas para analisar e criar insights de dados, as consultas sofrem constantes mudanças para executar dados complexos e pesados da maneira mais rápida e eficiente, permite flexibilidade e escalabilidade para diferentes volumes de dados, integração de dados de várias fontes para uma visão holística, e ferramentas avançadas para análises, visualização e machine learning. No entanto, elas também apresentam desafios, incluindo complexidade de implementação e manutenção, custos elevados dependendo da ferramenta, dificuldades no gerenciamento da qualidade e segurança dos dados, e a necessidade de ajustes frequentes na infraestrutura para lidar com o crescimento contínuo dos dados.

Para gerenciar esses dados, utilizam-se Data Lakes, Marts e Warehouses de forma complementar. O Data Lake armazena dados brutos de diversas fontes, sem estrutura definida, os Data Marts são repositorios menores que atendem analíses específicas, variando por exemlpo, de departamento para departamento enquanto, o Data Warehouse contém dados processados e estruturados para consultas e análises eficientes. Isso permite flexibilidade e escalabilidade no armazenamento inicial dos dados no Data Lake, e estrutura e desempenho otimizados para análise de negócios no Data Warehouse.

**Bases Transacionais**

São sistemas de gerenciamento e processamento de transações de dados em tempo real (OLTP - Online Transaction Processing), realiza operações de criação, leitura, atualização e exclusão de dados (CRUD) de maneira rápida e eficiente, suportam grande números de usuários simultâneos, mas mantendo a integridade dos dados e a baixa latência.

```
Propriedades ACID
Atomicidade: Assegura que todas as operações dentro de uma transação sejam concluídas com sucesso; se uma parte da transação falhar, toda a transação falha e o sistema retorna ao estado anterior.
Consistência: Garante que uma transação leve o banco de dados de um estado válido para outro estado válido, mantendo a integridade dos dados.
Isolamento: Garante que as transações sejam isoladas umas das outras, prevenindo interferências entre transações simultâneas.
Durabilidade: Assegura que, uma vez que uma transação é confirmada, as mudanças persistem mesmo em caso de falha do sistema.
```

**Big Query**

O Google BigQuery é um serviço de data warehouse totalmente gerenciado e altamente escalável, projetado para executar consultas SQL rápidas e eficazes em grandes volumes de dados. Ele oferece escalabilidade para lidar com simples consultas ou até com petabytes de dados, suporte para análise em tempo real, integração com ferramentas de BI (Business Intelligence) e recursos de machine learning integrados.

**Linguagens e Frameworks**

**Python** é uma linguagem de programação que foi criada com objetivo de ser facilmente lida e escrita, pode ser usada para vários fins, como desenvolvimento web, análise de dados, IA e até machine learning.

**Apache Spark** é um framework de processamento de dados em grande escala, seu diferencial é processar os dados em memória, resultando num desempenho muito rápido. Utilizando o *PySpark* como interface podemos utilizar a sintaxe e as bibliotecas da linguagem Python para  monitorar dados de sensores em tempo real para manutenção preditiva e monitoramento de condições ou grandes volumes de logs e detecção de anomalias, usar algoritmos de machine learning para recomendar produtos através do histórico do usuário.

**Apache Beam** é um modelo unificado para processamento de dados em lote e streaming, permite definir e executar uma pipeline consistente em diferentes plataformas e linguagens, utilizado em ETL, analisar e responder dados de streaming em tempo real e realizar cálculos complexos.

O **Google Dataflow** é um serviço da Google Cloud Plataform, ele executa as pipelines criadas com o Apache Beam, com processamento em streaming ou lote, com escabilidade automática(capacidade de lidar com grandes volumes de dados e ajustar recursos conforme necessário),balanceamento de carga e gerenciamento simples, facilitando a integração com outras ferramentas da Google Cloud.

**Apache Airflow** é uma plataforma de código aberto para orquestração e automação de workflows de dados. Criar, agendar e monitorar fluxos de trabalho programáveis em forma de DAGs (Directed Acyclic Graphs). Permite a automação de tarefas complexas de dados, como ETL (Extract, Transform, Load), processamento de dados em massa e integrações entre sistemas, facilitando a orquestração de pipelines de dados, ou seja, é possível visualizar, rastrear, emitir alertas e gerenciar tarefas em tempo real, além de reexecutar fluxos de trabalho falhos, realizar análises detalhadas dos dados processados.

- DAGs - Estrutura que define a relação e ordem que as tarefas devem ser executadas.

**Recursos Utilizados:**  
- Youtube
- Google Big Query
- Google Cloud Skills Boost
- Google Colab

**Desafios Encontrados:**  
A falta de uma experiência prática dificulta a compreensão de certos tópicos como os frameworks que vamos utilizar, mensageria também, mas provavelmente serão respondidas nos próximos passos.

**Feedback e Ajustes:**  
Como uma melhoria futura, era interessante adicionar exemplos práticos do uso dessas tecnologias e talvez até atividades(Projeto/Quiz) para melhor entendimento do assunto.

**Próximos Passos:**  
Continuar aprofundando sobre os temas na trilha de conhecimento e realizar projetos para melhor compreensão do assunto.